;; Object nn/
;; SEMANTICDB Tags save file
(semanticdb-project-database-file "nn/"
  :tables (list 
   (semanticdb-table "init.py"
    :major-mode 'python-mode
    :tags 
        '( ("math" include nil nil [1 12])
            ("random" include nil nil [13 26])
            ("torch" include nil nil [28 40])
            ("torch.autograd" include nil nil [41 76])
            ("calculate_gain" function
               (:documentation "Return the recommended gain value for the given nonlinearity function.
    The values are as follows:

    ============ ==========================================
    nonlinearity gain
    ============ ==========================================
    linear       :math:`1`
    conv{1,2,3}d :math:`1`
    sigmoid      :math:`1`
    tanh         :math:`5 / 3`
    relu         :math:`\\sqrt{2}`
    leaky_relu   :math:`\\sqrt{2 / (1 + negative\\_slope^2)}`
    ============ ==========================================

    Args:
        nonlinearity: the nonlinear function (`nn.functional` name)
        param: optional parameter for the nonlinear function

    Examples:
        >>> gain = nn.init.calculate_gain('leaky_relu')
    "
                :arguments 
                  ( ("nonlinearity" variable nil (reparse-symbol function_parameters) [98 110])
                    ("param" variable nil (reparse-symbol function_parameters) [112 117]))                  )
                nil [79 1718])
            ("uniform" function
               (:documentation "Fills the input Tensor or Variable with values drawn from the uniform
    distribution :math:`U(a, b)`.

    Args:
        tensor: an n-dimensional torch.Tensor or autograd.Variable
        a: the lower bound of the uniform distribution
        b: the upper bound of the uniform distribution

    Examples:
        >>> w = torch.Tensor(3, 5)
        >>> nn.init.uniform(w)
    "
                :arguments 
                  ( ("tensor" variable nil (reparse-symbol function_parameters) [1732 1738])
                    ("a" variable nil (reparse-symbol function_parameters) [1740 1741])
                    ("b" variable nil (reparse-symbol function_parameters) [1745 1746]))                  )
                nil [1720 2271])
            ("normal" function
               (:documentation "Fills the input Tensor or Variable with values drawn from the normal
    distribution :math:`N(mean, std)`.

    Args:
        tensor: an n-dimensional torch.Tensor or autograd.Variable
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution

    Examples:
        >>> w = torch.Tensor(3, 5)
        >>> nn.init.normal(w)
    "
                :arguments 
                  ( ("tensor" variable nil (reparse-symbol function_parameters) [2284 2290])
                    ("mean" variable nil (reparse-symbol function_parameters) [2292 2296])
                    ("std" variable nil (reparse-symbol function_parameters) [2300 2303]))                  )
                nil [2273 2847])
            ("constant" function
               (:documentation "Fills the input Tensor or Variable with the value `val`.

    Args:
        tensor: an n-dimensional torch.Tensor or autograd.Variable
        val: the value to fill the tensor with

    Examples:
        >>> w = torch.Tensor(3, 5)
        >>> nn.init.constant(w)
    "
                :arguments 
                  ( ("tensor" variable nil (reparse-symbol function_parameters) [2862 2868])
                    ("val" variable nil (reparse-symbol function_parameters) [2870 2873]))                  )
                nil [2849 3279])
            ("eye" function
               (:documentation "Fills the 2-dimensional input Tensor or Variable with the identity
    matrix. Preserves the identity of the inputs in Linear layers, where as
    many inputs are preserved as possible.

    Args:
        tensor: a 2-dimensional torch.Tensor or autograd.Variable

    Examples:
        >>> w = torch.Tensor(3, 5)
        >>> nn.init.eye(w)
    "
                :arguments 
                  ( ("tensor" variable nil (reparse-symbol function_parameters) [3289 3295]))                  )
                nil [3281 3912])
            ("dirac" function
               (:documentation "Fills the {3, 4, 5}-dimensional input Tensor or Variable with the Dirac
    delta function. Preserves the identity of the inputs in Convolutional
    layers, where as many input channels are preserved as possible.

    Args:
        tensor: a {3, 4, 5}-dimensional torch.Tensor or autograd.Variable

    Examples:
        >>> w = torch.Tensor(3, 16, 5, 5)
        >>> nn.init.dirac(w)
    "
                :arguments 
                  ( ("tensor" variable nil (reparse-symbol function_parameters) [3924 3930]))                  )
                nil [3914 5065])
            ("_calculate_fan_in_and_fan_out" function (:arguments 
              ( ("tensor" variable nil (reparse-symbol function_parameters) [5101 5107]))              ) nil [5067 5729])
            ("xavier_uniform" function
               (:documentation "Fills the input Tensor or Variable with values according to the method
    described in \"Understanding the difficulty of training deep feedforward
    neural networks\" - Glorot, X. & Bengio, Y. (2010), using a uniform
    distribution. The resulting tensor will have values sampled from
    :math:`U(-a, a)` where
    :math:`a = gain \\\\times \\sqrt{2 / (fan\\_in + fan\\_out)} \\\\times \\sqrt{3}`.
    Also known as Glorot initialisation.

    Args:
        tensor: an n-dimensional torch.Tensor or autograd.Variable
        gain: an optional scaling factor

    Examples:
        >>> w = torch.Tensor(3, 5)
        >>> nn.init.xavier_uniform(w, gain=nn.init.calculate_gain('relu'))
    "
                :arguments 
                  ( ("tensor" variable nil (reparse-symbol function_parameters) [5750 5756])
                    ("gain" variable nil (reparse-symbol function_parameters) [5758 5762]))                  )
                nil [5731 6795])
            ("xavier_normal" function
               (:documentation "Fills the input Tensor or Variable with values according to the method
    described in \"Understanding the difficulty of training deep feedforward
    neural networks\" - Glorot, X. & Bengio, Y. (2010), using a normal
    distribution. The resulting tensor will have values sampled from
    :math:`N(0, std)` where
    :math:`std = gain \\\\times \\sqrt{2 / (fan\\_in + fan\\_out)}`.
    Also known as Glorot initialisation.

    Args:
        tensor: an n-dimensional torch.Tensor or autograd.Variable
        gain: an optional scaling factor

    Examples:
        >>> w = torch.Tensor(3, 5)
        >>> nn.init.xavier_normal(w)
    "
                :arguments 
                  ( ("tensor" variable nil (reparse-symbol function_parameters) [6815 6821])
                    ("gain" variable nil (reparse-symbol function_parameters) [6823 6827]))                  )
                nil [6797 7725])
            ("_calculate_correct_fan" function (:arguments 
              ( ("tensor" variable nil (reparse-symbol function_parameters) [7754 7760])
                ("mode" variable nil (reparse-symbol function_parameters) [7762 7766]))              ) nil [7727 8075])
            ("kaiming_uniform" function
               (:documentation "Fills the input Tensor or Variable with values according to the method
    described in \"Delving deep into rectifiers: Surpassing human-level
    performance on ImageNet classification\" - He, K. et al. (2015), using a
    uniform distribution. The resulting tensor will have values sampled from
    :math:`U(-bound, bound)` where
    :math:`bound = \\sqrt{2 / ((1 + a^2) \\\\times fan\\_in)} \\\\times \\sqrt{3}`.
    Also known as He initialisation.

    Args:
        tensor: an n-dimensional torch.Tensor or autograd.Variable
        a: the negative slope of the rectifier used after this layer (0 for ReLU
            by default)
        mode: either 'fan_in' (default) or 'fan_out'. Choosing `fan_in`
            preserves the magnitude of the variance of the weights in the
            forward pass. Choosing `fan_out` preserves the magnitudes in the
            backwards pass.

    Examples:
        >>> w = torch.Tensor(3, 5)
        >>> nn.init.kaiming_uniform(w, mode='fan_in')
    "
                :arguments 
                  ( ("tensor" variable nil (reparse-symbol function_parameters) [8097 8103])
                    ("a" variable nil (reparse-symbol function_parameters) [8105 8106])
                    ("mode" variable nil (reparse-symbol function_parameters) [8110 8114]))                  )
                nil [8077 9485])
            ("kaiming_normal" function
               (:documentation "Fills the input Tensor or Variable with values according to the method
    described in \"Delving deep into rectifiers: Surpassing human-level
    performance on ImageNet classification\" - He, K. et al. (2015), using a
    normal distribution. The resulting tensor will have values sampled from
    :math:`N(0, std)` where
    :math:`std = \\sqrt{2 / ((1 + a^2) \\\\times fan\\_in)}`. Also known as He
    initialisation.

    Args:
        tensor: an n-dimensional torch.Tensor or autograd.Variable
        a: the negative slope of the rectifier used after this layer (0 for ReLU
            by default)
        mode: either 'fan_in' (default) or 'fan_out'. Choosing `fan_in`
            preserves the magnitude of the variance of the weights in the
            forward pass. Choosing `fan_out` preserves the magnitudes in the
            backwards pass.

    Examples:
        >>> w = torch.Tensor(3, 5)
        >>> nn.init.kaiming_normal(w, mode='fan_out')
    "
                :arguments 
                  ( ("tensor" variable nil (reparse-symbol function_parameters) [9506 9512])
                    ("a" variable nil (reparse-symbol function_parameters) [9514 9515])
                    ("mode" variable nil (reparse-symbol function_parameters) [9519 9523]))                  )
                nil [9487 10773])
            ("orthogonal" function
               (:documentation "Fills the input Tensor or Variable with a (semi) orthogonal matrix, as
    described in \"Exact solutions to the nonlinear dynamics of learning in deep
    linear neural networks\" - Saxe, A. et al. (2013). The input tensor must have
    at least 2 dimensions, and for tensors with more than 2 dimensions the
    trailing dimensions are flattened.

    Args:
        tensor: an n-dimensional torch.Tensor or autograd.Variable, where n >= 2
        gain: optional scaling factor

    Examples:
        >>> w = torch.Tensor(3, 5)
        >>> nn.init.orthogonal(w)
    "
                :arguments 
                  ( ("tensor" variable nil (reparse-symbol function_parameters) [10790 10796])
                    ("gain" variable nil (reparse-symbol function_parameters) [10798 10802]))                  )
                nil [10775 12072])
            ("sparse" function
               (:documentation "Fills the 2D input Tensor or Variable as a sparse matrix, where the
    non-zero elements will be drawn from the normal distribution
    :math:`N(0, 0.01)`, as described in \"Deep learning via
    Hessian-free optimization\" - Martens, J. (2010).

    Args:
        tensor: an n-dimensional torch.Tensor or autograd.Variable
        sparsity: The fraction of elements in each column to be set to zero
        std: the standard deviation of the normal distribution used to generate
        the non-zero values

    Examples:
        >>> w = torch.Tensor(3, 5)
        >>> nn.init.sparse(w, sparsity=0.1)
    "
                :arguments 
                  ( ("tensor" variable nil (reparse-symbol function_parameters) [12085 12091])
                    ("sparsity" variable nil (reparse-symbol function_parameters) [12093 12101])
                    ("std" variable nil (reparse-symbol function_parameters) [12103 12106]))                  )
                nil [12074 13330]))          
    :file "init.py"
    :pointmax 13330
    :fsize 13329
    :lastmodtime '(23105 45917 712911 0)
    :unmatched-syntax '((RETURN 8028 . 8034) (IF 8042 . 8044) (ELSE 8062 . 8066))
    )
   (semanticdb-table "functional.py"
    :major-mode 'python-mode
    :tags nil
    :file "functional.py"
    :pointmax 66657
    :fsize 66656
    :lastmodtime '(23105 45917 713911 0)
    :unmatched-syntax 'nil
    )
   (semanticdb-table "__init__.py"
    :major-mode 'python-mode
    :tags 
        '( ("Parameter" include nil nil [40 56])
            ("DataParallel" include nil nil [72 91])
            ("init" include nil nil [99 110])
            ("utils" include nil nil [118 130]))          
    :file "__init__.py"
    :pointmax 131
    :fsize 130
    :lastmodtime '(23105 45917 704910 0)
    :unmatched-syntax '((FROM 111 . 115) (PERIOD 116 . 117) (FROM 92 . 96) (PERIOD 97 . 98) (FROM 57 . 61) (PERIOD 62 . 63) (FROM 24 . 28) (PERIOD 29 . 30) (IMPORT 15 . 21) (MULT 22 . 23) (PERIOD 6 . 7))
    )
   )
  :file "!usr!lib64!python2.7!site-packages!torch!nn!semantic.cache"
  :semantic-tag-version "2.0"
  :semanticdb-version "2.2"
  )
